{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "pyLDAvis.enable_notebook()\n",
    "# LDA csomagok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gensim==3.8.1 --user #ez a verzió támogatja a wrappers-t, kell a pyLDAvis-hez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "os.environ.update({'MALLET_HOME':r'C:/mallet/mallet-2.0.8/'})\n",
    "mallet_path = r'C:/mallet/mallet-2.0.8/bin/mallet.bat'\n",
    "#MALLET útvonal megadás"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.corpora import MalletCorpus\n",
    "from gensim.models import CoherenceModel\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import join, isfile\n",
    "import logging\n",
    "import codecs\n",
    "import tqdm\n",
    "import numpy as np\n",
    "#szükséges csomagok behívása"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#végleges preprocess fájlok behívása\n",
    "in_folder = 'C:/Users/zabor/Desktop/preprocess/adatok/Data/txts_final'\n",
    "in_path = os.path.join(in_folder)\n",
    "in_files = [f for f in os.listdir(in_path) if os.path.isfile(os.path.join(in_path, f))]\n",
    "\n",
    "#összes fájl listába rakása\n",
    "txt = []\n",
    "for filename in tqdm.tqdm(in_files):\n",
    "    try:\n",
    "        with open(os.path.join(in_path, filename), 'r', encoding='utf-8') as infile:\n",
    "            article = infile.read()\n",
    "        txt.append(article)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"wonât\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"canât\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"nât\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"âre\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"âs\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"âd\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"âll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"ât\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"âve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"âm\", \" am\", phrase)\n",
    "    return phrase\n",
    "#összevonások kezelése (can't -> can + not), beépített contractions.fix nem ismerte föl 'â' miatt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt2 = []\n",
    "for i in txt:\n",
    "    txt2.append(decontracted(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_list = [t.split() for t in txt2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename=\"C:/Users/zabor/Desktop/LDA/gensim.log\",\n",
    "                   format=\"%(asctime)s:%(levelname)s:%(message)s\",\n",
    "                   level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import operator\n",
    "from collections import Counter\n",
    "from os import listdir, remove\n",
    "\n",
    "\n",
    "##### GET WORDFREQ\n",
    "\n",
    "in_path = 'C:/Users/zabor/Desktop/preprocess/adatok/Data/txts_final'\n",
    "corpus = []\n",
    "files = sorted([f for f in listdir(in_path) if isfile(join(in_path, f))])\n",
    "for f in files:\n",
    "    try:\n",
    "        txt = codecs.open(join(in_path, f), 'r', 'utf-8').read()\n",
    "        txt = decontracted(txt)\n",
    "        txt = txt.strip().split()\n",
    "        txt = [w.lower() for w in txt if len(w) > 3]\n",
    "        corpus.append(txt)\n",
    "    except Exception as e:\n",
    "        print(f, e)\n",
    "\n",
    "wordlist = list(itertools.chain(*corpus))\n",
    "wfreq = Counter(wordlist)\n",
    "sorted_wfreq = sorted(wfreq.items(),\n",
    "                      key=operator.itemgetter(1),\n",
    "                      reverse=True)\n",
    "\n",
    "with codecs.open('C:/Users/zabor/Desktop/LDA/wfreq.tsv', 'w', 'utf-8') as f:\n",
    "    for e in sorted_wfreq:\n",
    "        if e[1] > 3 and e[1] < 10000:\n",
    "            o = e[0] + '\\t' + str(e[1]) + '\\n'\n",
    "            f.write(o)\n",
    "\n",
    "\n",
    "\n",
    "##### GET DOCFREQ\n",
    "\n",
    "\n",
    "wd_docfreq = {}\n",
    "for d in corpus:\n",
    "    d = list(set(d))\n",
    "    for w in d:\n",
    "        if w not in wd_docfreq.keys():\n",
    "            wd_docfreq[w] = 1\n",
    "        else:\n",
    "            wd_docfreq[w] += 1\n",
    "\n",
    "sorted_docfreq = sorted(wd_docfreq.items(),\n",
    "                        key=operator.itemgetter(1),\n",
    "                        reverse=True)\n",
    "\n",
    "with codecs.open('C:/Users/zabor/Desktop/LDA/docfreq.tsv', 'w', 'utf-8') as f:\n",
    "    for e in sorted_docfreq:\n",
    "        o = e[0] + '\\t' + str((e[1]/len(corpus))) + '\\n'\n",
    "        f.write(o)\n",
    "\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "#####                          Content words                            #####\n",
    "#############################################################################\n",
    "\n",
    "\n",
    "content_words = []\n",
    "of = codecs.open('C:/Users/zabor/Desktop/LDA/contentwords.tsv', 'w', 'utf-8')\n",
    "with codecs.open('C:/Users/zabor/Desktop/LDA/docfreq.tsv', 'r', 'utf-8') as input_file:\n",
    "    for l in input_file:\n",
    "        wd, docfreq = l.strip().split('\\t')\n",
    "        docfreq = float(docfreq)\n",
    "        if docfreq > 0.00010:\n",
    "            content_words.append(wd)\n",
    "            o = wd + '\\t' + str(docfreq) + '\\n'\n",
    "            of.write(o)\n",
    "of.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopszavak\n",
    "stopwords = []\n",
    "\n",
    "\n",
    "# content words kimentése\n",
    "out_path = 'C:/Users/zabor/Desktop/LDA/Data/txts_contwrd'\n",
    "folder = [f for f in listdir(out_path) if isfile(join(out_path, f))]\n",
    "if len(folder) != 0:\n",
    "    for f in folder:\n",
    "        remove(join(out_path, f))\n",
    "\n",
    "in_path = 'C:/Users/zabor/Desktop/preprocess/adatok/Data/txts_final'\n",
    "files = sorted([f for f in listdir(in_path) if isfile(join(in_path, f))])\n",
    "file=0\n",
    "for f in files:\n",
    "    with codecs.open(join(out_path, files[file]), 'w', 'utf-8') as out_file:\n",
    "        try:\n",
    "            txt = codecs.open(join(in_path, f), 'r', 'utf-8').read()\n",
    "            txt = txt.strip().split()\n",
    "            for cw in txt:\n",
    "                if cw in content_words and cw not in stopwords:\n",
    "                    out_file.write(cw + \" \")\n",
    "        except Exception as e:\n",
    "            print(files[file], e)\n",
    "    file += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from multiprocessing import freeze_support\n",
    "from datetime import datetime\n",
    "# még több csomag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "#####                          LDA                                      #####\n",
    "#############################################################################\n",
    "\n",
    "# Save document to topic Excel files for each run?\n",
    "save_document_to_topic = False\n",
    "\n",
    "# Save visualization files for each run?\n",
    "save_visualization = False\n",
    "\n",
    "# Define in_folder for corpus files.\n",
    "in_folder_corpus = 'C:/Users/zabor/Desktop/LDA/Data/txts_contwrd'\n",
    "in_folder_original = 'C:/Users/zabor/Desktop/preprocess/adatok/Data/Nolink_txts'\n",
    "\n",
    "# TODO: egy közös fájlba is íródjanak ki a koherencia értékek.\n",
    "# TODO: kiíratni a coherence mappába a korpuszhoz használt fájlneveket egy tsv-be.\n",
    "# TODO: a kiírt fájlba, amiben látszanak a topikok és a dokumentumok, ott a dokumentum szövege ne listaként, hanem stringként íródjon ki + a fájlnév külön oszlopban legyen.\n",
    "\n",
    "# Minimal article length (words).\n",
    "minimal_length = 5\n",
    "\n",
    "# Settings.\n",
    "c_times = 5  # number of times to be run\n",
    "c_start = 5  # starting topic number\n",
    "c_limit = 20  # max. topic number\n",
    "c_step = 1  # stepping interval\n",
    "\n",
    "# Outfile path for writing out model coherences and models.\n",
    "outfile_path = os.path.join('C:/Users/zabor/Desktop/LDA/Data',\n",
    "                            'C:/Users/zabor/Desktop/LDA/Data/model',\n",
    "                            'C:/Users/zabor/Desktop/LDA/Data/coherence')\n",
    "\n",
    "# Mallet settings.\n",
    "# mallet_path = os.path.join('C:', 'Work', 'mallet', 'mallet-2.0.8', 'bin', 'mallet')\n",
    "mallet_path = r'C:/mallet/mallet-2.0.8/bin/mallet.bat'\n",
    "os.environ.update({'MALLET_HOME':r'C:/mallet/mallet-2.0.8/'})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    freeze_support()\n",
    "\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=1):\n",
    "        \"\"\"\n",
    "        Compute c_v coherence for various number of topics\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        dictionary : Gensim dictionary\n",
    "        corpus : Gensim corpus\n",
    "        texts : List of input texts\n",
    "        limit : Max num of topics\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        model_list : List of LDA topic models\n",
    "        coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "        \"\"\"\n",
    "        coherence_values = []\n",
    "        model_list = []\n",
    "        rs = 0\n",
    "        for num_topics in tqdm.tqdm(range(start, limit, step), leave=False):\n",
    "            model = gensim.models.wrappers.LdaMallet(mallet_path,\n",
    "                                                     corpus=corpus,\n",
    "                                                     num_topics=num_topics,\n",
    "                                                     workers=3,\n",
    "                                                     id2word=dictionary,\n",
    "                                                     optimize_interval=10,\n",
    "                                                     #rs+run_number=13 ha t18_r01 a modell\n",
    "                                                     random_seed=rs+run_number)\n",
    "            rs += 1\n",
    "            model_list.append(model)\n",
    "            coherencemodel = CoherenceModel(model=model,\n",
    "                                            texts=texts,\n",
    "                                            dictionary=dictionary,\n",
    "                                            processes=1,\n",
    "                                            coherence='c_v')\n",
    "            coherence_value = coherencemodel.get_coherence()\n",
    "            coherence_values.append(coherence_value)\n",
    "\n",
    "            # Save model.\n",
    "            model.save(os.path.join(outfile_path, 'model_topics_t'+str(\"{:02d}\".format(num_topics))+'_r'+str(\"{:02d}\".format(run_number+1))+'.model'))\n",
    "\n",
    "            # Save visualization.\n",
    "            if save_visualization:\n",
    "                lda = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(model)\n",
    "                vis = gensimvis.prepare(lda, corpus, dictionary, sort_topics=False)\n",
    "                pyLDAvis.save_html(vis, os.path.join(outfile_path, 'visualization_t'+str(\"{:02d}\".format(num_topics))+'_r'+str(\"{:02d}\".format(run_number+1))+'.html'))\n",
    "\n",
    "            # Save document to topic Excel.\n",
    "            if save_document_to_topic:\n",
    "                df_topic_sents_keywords = format_topics_sentences(ldamodel=model, corpus=corpus, texts=original_documents)\n",
    "                df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "                df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "                df_dominant_topic.to_excel(os.path.join(outfile_path, 'document_dominant_topics_t'+str(\"{:02d}\".format(num_topics))+'_r'+str(\"{:02d}\".format(run_number+1))+'.xlsx'))\n",
    "\n",
    "            print('\\nCurrent model coherence is '+str(coherence_value)+' with '+str(num_topics)+' topics.')\n",
    "        return model_list, coherence_values\n",
    "\n",
    "\n",
    "    def format_topics_sentences(ldamodel, corpus, texts):\n",
    "        # Init output\n",
    "        sent_topics_df = pd.DataFrame()\n",
    "\n",
    "        # Get main topic in each document\n",
    "        for i, row in enumerate(ldamodel[corpus]):\n",
    "            row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "            # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "            for j, (topic_num, prop_topic) in enumerate(row):\n",
    "                if j == 0:  # => dominant topic\n",
    "                    wp = ldamodel.show_topic(topic_num)\n",
    "                    topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                    sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic, 4), topic_keywords]), ignore_index=True)\n",
    "                else:\n",
    "                    break\n",
    "        sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "        # Add original text to the end of the output\n",
    "        contents = pd.Series(texts)\n",
    "        sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "        return sent_topics_df\n",
    "\n",
    "    # Load corpus files.\n",
    "    in_path = 'C:/Users/zabor/Desktop/LDA/Data/txts_contwrd'\n",
    "    in_files = [f for f in os.listdir(in_path) if os.path.isfile(os.path.join(in_path, f))]\n",
    "\n",
    "    # Add all files to a list.\n",
    "    txt = []\n",
    "    files_used = []\n",
    "    for filename in tqdm.tqdm(in_files):\n",
    "        with open(os.path.join(in_path, filename), 'r', encoding='utf-8') as infile:\n",
    "            article = infile.read()\n",
    "        if len(article.split(' ')) > minimal_length:\n",
    "            txt.append(article)\n",
    "            files_used.append(filename)\n",
    "\n",
    "    # Create lists containing the texts and the words.\n",
    "    txts = []\n",
    "    for t in txt:\n",
    "        txts.append(t.strip().split())\n",
    "    words = list(itertools.chain(*txts))\n",
    "\n",
    "    # Load original documents.\n",
    "    original_documents_folder = os.path.join(in_folder_original)\n",
    "    original_document_files = [f for f in os.listdir(original_documents_folder) if os.path.isfile(os.path.join(original_documents_folder, f))]\n",
    "\n",
    "    # Add all files to a list.\n",
    "    original_documents = []\n",
    "    for filename in tqdm.tqdm(original_document_files):\n",
    "        if filename in files_used:\n",
    "            with open(os.path.join(original_documents_folder, filename), 'r', encoding='utf-8') as infile:\n",
    "                article = infile.read()\n",
    "            lst = [str(filename + ': ' + article)]\n",
    "            original_documents.append(lst)\n",
    "\n",
    "    # Create Dictionary\n",
    "    dictionary = corpora.Dictionary(txts)\n",
    "    dictionary.save('dictionary.dict')\n",
    "\n",
    "    # Create corpus\n",
    "    corpus = [dictionary.doc2bow(text) for text in txts]\n",
    "\n",
    "    # Write corpus in Mallet format to disk\n",
    "    MalletCorpus.serialize(\"corpus.mallet\", corpus, dictionary)\n",
    "\n",
    "    # Read corpus\n",
    "    # corpus = MalletCorpus(\"data/corpus/corpus.mallet\")\n",
    "\n",
    "    # Run models.\n",
    "    for run_number in tqdm.tqdm(range(0, c_times)):\n",
    "        print('Starting run number: '+str(\"{:02d}\".format(run_number+1))+'.')\n",
    "        model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus, texts=txts, start=c_start, limit=c_limit, step=c_step)\n",
    "\n",
    "        with open(os.path.join(outfile_path, 'coherence_values_r'+str(\"{:02d}\".format(run_number+1))+'.tsv'), 'w', encoding='utf-8') as outfile:\n",
    "            c = 0\n",
    "            for value in coherence_values:\n",
    "                outfile.write(str(model_list[c].num_topics)+'\\t'+str(value)+'\\n')\n",
    "                c += 1\n",
    "\n",
    "        # Create plot.\n",
    "        x = range(c_start, c_limit, c_step)\n",
    "        plt.plot(x, coherence_values)\n",
    "        plt.xlabel(\"Num Topics\")\n",
    "        plt.ylabel(\"Coherence score\")\n",
    "        plt.savefig(os.path.join(outfile_path, 'coherence_values_r'+str(\"{:02d}\".format(run_number+1))+'.png'))\n",
    "        # plt.show()\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    print('All finished.\\nDuration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.LdaModel.load('C:/Users/zabor/Desktop/LDA/Data/model/coherence/model_topics_t18_r02.model')\n",
    "#optimális modell betöltése"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lda = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(model)\n",
    "vis = gensimvis.prepare(model_lda, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.save_html(vis, 'C:/Users/zabor/Desktop/LDA/Data/model_topics_t18_r02.html')\n",
    "# LDA model vizualizáció kimentése (HTML-ben)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

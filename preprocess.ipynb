{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/zabor/Desktop/preprocess/depression_data_2020covid_deprred_clean_v3.csv\", sep = \";\", encoding = \"utf8\", error_bad_lines=False)\n",
    "#adatbázis első beolvasása"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.shape\n",
    "#adatbázis szerkezete (sor és oszlop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "#adatbázis fejléc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df[df[\"onlyday\"] >= \"2020-03-11\"]\n",
    "#időszakra szűkítés, 2020.03.11. utáni adatok megtartása"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_new.reset_index(drop=True)\n",
    "#újra indexelés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.shape\n",
    "#új adatbázis szerkezet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {}\n",
    "for i in range(len(df_new[\"id\"])):\n",
    "    dictionary[df_new[\"id\"][i]] = df_new[\"post_content\"][i]\n",
    "print(list(dictionary.items())[0])\n",
    "#szótár készítése id - szöveg formában"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.columns\n",
    "#adatbázis oszlopai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new[\"post_content\"][20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new[\"post_content\"] = df_new[\"post_content\"].str.replace(\"â€™\", \"'\")\n",
    "# â€™ karakterek cseréje '-re, összevonások későbbi kezelése miatt fontos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new[\"post_content\"][20]\n",
    "#ellenőrzés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary2 = {}\n",
    "for key, value in dictionary.items():\n",
    "    dictionary2.setdefault(value, set()).add(key)\n",
    "\n",
    "dupl = [list(values) for key, values in dictionary2.items() if len(values) > 1] \n",
    "for i in range(len(dupl)):\n",
    "    del dupl[i][-1]\n",
    "#szótár készítése duplikációk eltávolítására"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dupl)\n",
    "# hány darab duplikáció van"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dupl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove = []\n",
    "for i in range(len(dupl)):\n",
    "    for j in range(len(dupl[i])):\n",
    "        remove.append(dupl[i][j])\n",
    "#duplikációk törlése"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new2 = df_new[~df_new[\"id\"].isin(remove)]\n",
    "df_new2 = df_new2.reset_index(drop=True)\n",
    "df_new2.shape\n",
    "#újra indexelés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urlextract import URLExtract\n",
    "extractor = URLExtract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_txt = []\n",
    "for file_name in df_new2[\"post_content\"]:\n",
    "    full = str(file_name)\n",
    "    full = full.replace('\\n', ' ')\n",
    "    urls = extractor.find_urls(full)\n",
    "    if len(urls) > 0:\n",
    "        for u in urls:\n",
    "            full = full.replace(u, \"\")\n",
    "    id_txt.append(full)\n",
    "#linkek eltávolítása"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(id_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new2[\"nolink\"]=id_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new2.to_csv(\"C:/Users/zabor/Desktop/preprocess/depression_data_2020covid_deprred_clean_v4.csv\", index=False)\n",
    "df_new2.to_excel(\"C:/Users/zabor/Desktop/preprocess/depression_data_2020covid_deprred_clean_v4.xlsx\", index=False)\n",
    "#új adatbázis kimentése"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"C:/Users/zabor/Desktop/preprocess/depression_data_2020covid_deprred_clean_v4.csv\", sep = \",\", encoding = \"utf8\", error_bad_lines=False)\n",
    "#kimentett adatbázis beolvasása (ne kelljen elölről futtatni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_hossz = []\n",
    "for i in range(len(df[\"nolink\"])):\n",
    "    txt_hossz.append(len(str(df[\"nolink\"][i]).split()))\n",
    "df['txt_hossz'] = txt_hossz\n",
    "df_hossz = df[df['txt_hossz'] >= 20]\n",
    "df_hossz = df_hossz.reset_index(drop = True)\n",
    "#20 szó alattiak törlése"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hossz[\"post_content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hossz.to_excel(\"C:/Users/zabor/Desktop/preprocess/depression_data_2020covid_deprred_clean_v5.xlsx\", index=False)\n",
    "df_hossz.to_csv(\"C:/Users/zabor/Desktop/preprocess/depression_data_2020covid_deprred_clean_v5.csv\", index=False)\n",
    "# új adatbázis kimentése"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import tqdm\n",
    "import os\n",
    "import re\n",
    "import contractions #ehhez pip install kell\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import nltk\n",
    "nltk.download() #felugró ablak\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "#csomagok letöltése"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = StanfordNERTagger('C:/Users/zabor/Desktop/preprocess/adatok/stanford-ner-2020-11-17/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "                       'C:/Users/zabor/Desktop/preprocess/adatok/stanford-ner-2020-11-17/stanford-ner.jar')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stop_en = stopwords.words('english')\n",
    "#nevezett entitás felismerés (NER tagger), lemmatizálás, stopszavazás behívása"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_list = []\n",
    "person_full = []\n",
    "\n",
    "def get_human_names(text):\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    pos = nltk.pos_tag(tokens)\n",
    "    sentt = nltk.ne_chunk(pos, binary=False)\n",
    "\n",
    "    person = []\n",
    "    name = \"\"\n",
    "    for subtree in sentt.subtrees(filter=lambda t: t.label() == 'PERSON'):\n",
    "        for leaf in subtree.leaves():\n",
    "            person.append(leaf[0])\n",
    "        if len(person) > 1: #magányos vezetéknevek kigyűjtésének elkerülése\n",
    "            for part in person:\n",
    "                name += part + ' '\n",
    "            person_full.append(name[:-1])\n",
    "            if name[:-1] not in person_list:\n",
    "                person_list.append(name[:-1])\n",
    "            name = ''\n",
    "        person = []\n",
    "#nevekből lista készítés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hossz.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_hossz)):\n",
    "    f = open(\"Data/Nolink_txts/\"+str(i), \"w\", encoding='utf-8')\n",
    "    f.write(str(df_hossz[\"nolink\"][i]))\n",
    "    f.close()\n",
    "#szövegek kimentése fájlokba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#nevek megkeresése a fájlokban\n",
    "in_path = os.path.join('Data', 'Nolink_txts')\n",
    "in_files = [f for f in os.listdir(in_path) if os.path.isfile(os.path.join(in_path, f))]\n",
    "\n",
    "for file in tqdm.tqdm(in_files):\n",
    "    try:\n",
    "        with open(os.path.join(in_path, file), 'r', encoding='utf-8') as article:\n",
    "            article_text = article.read()\n",
    "        get_human_names(article_text)\n",
    "    except Exception as e:\n",
    "        print(file, e)\n",
    "\n",
    "#nevek kiírása\n",
    "ner_file = open(os.path.join('Data', 'ner.tsv'), 'w', encoding='utf-8')\n",
    "for name in tqdm.tqdm(person_list):\n",
    "    namelist = name.split()\n",
    "    line = \" \".join(namelist) + \"\\t\" +\"_\".join(namelist) + \"\\n\"\n",
    "    line = line.lower()\n",
    "    ner_file.write(line)\n",
    "ner_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list = open(os.path.join('Data', 'ner_nr.tsv'), 'r',\n",
    "                   encoding='utf-8').readlines()\n",
    "\n",
    "illness_list = open(os.path.join('Data', 'betegseg.tsv'), 'r',\n",
    "                    encoding='utf-8').readlines()\n",
    "#fájlok soronként beolvasása a két változóba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_list = name_list + illness_list\n",
    "\n",
    "\n",
    "replace_list = open(os.path.join('Data', 'betegseg.tsv'), 'r',\n",
    "                    encoding='utf-8').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace = []\n",
    "for r in replace_list:\n",
    "    w1, w2 = r.strip().split('\\t')\n",
    "    tup = (w1, w2)\n",
    "    replace.append(tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_names(text, r=replace):\n",
    "    text = \" \" + text + \" \"\n",
    "    for i in r:\n",
    "        text = text.replace(\" \" + i[0] + \" \", \" \" + i[1] + \" \")\n",
    "    return text\n",
    "#szóközös szavak cseréje alsóvonalas szópárokra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = os.path.join('Data', 'txts_ner')\n",
    "folder = [f for f in os.listdir(out_path) if os.path.isfile(os.path.join(out_path, f))]\n",
    "if len(folder) != 0:\n",
    "    for f in folder:\n",
    "        os.remove(os.path.join(out_path, f))\n",
    "#txts_ner mappából régi fájlok törlése (ha vannak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 0\n",
    "for filename in tqdm.tqdm(in_files):\n",
    "    try:\n",
    "        article = open(os.path.join(in_path, filename), 'r', encoding='utf-8').read()\n",
    "        article = article.lower()\n",
    "        article = re.sub(r'[^\\'^\\w\\s]', '', article)\n",
    "        names = filter_names(article)\n",
    "        with open(os.path.join(out_path, in_files[file]), 'w', encoding='utf-8') as out_file:\n",
    "            out_file.write(names)\n",
    "    except Exception as e:\n",
    "        print(in_files[file], e)\n",
    "    file += 1\n",
    "#fájlok beolvasása, majd szavak kicserélése betegseg.tsv egyezés alapján és kimentésük a txts_ner mappába"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_path = os.path.join('Data', 'txts_ner')\n",
    "in_files = [f for f in os.listdir(in_path) if os.path.isfile(os.path.join(in_path, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_contractions(text):\n",
    "    fixed = contractions.fix(text)\n",
    "    return fixed\n",
    "#angol összevonások kezelése pl.: you're -> you are ------> LDA munkafüzetben még kiegészítve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "#szófajok meghatározása: határozószavak, főnevek, igék, melléknevek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_filter(text):\n",
    "    proc_list = []\n",
    "    doc = text.strip()\n",
    "    for w in nltk.word_tokenize(doc):\n",
    "        w = w.lower()\n",
    "        tok_lemma = lemmatizer.lemmatize(w, get_wordnet_pos(w))\n",
    "        proc_list.append(tok_lemma)\n",
    "    return ' '.join(proc_list)\n",
    "#szavak stemmelése"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path_lem = os.path.join('Data', 'txts_stempos')\n",
    "folder = [f for f in os.listdir(out_path_lem) if os.path.isfile(os.path.join(out_path_lem, f))]\n",
    "folder_set = set(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in tqdm.tqdm(in_files):\n",
    "    if file in folder_set:\n",
    "        continue\n",
    "    try:\n",
    "        with open(os.path.join(in_path, file), 'r', encoding='utf-8') as article:\n",
    "            article_text = article.read()\n",
    "        fixed_text = fix_contractions(article_text)\n",
    "        filtered = stem_filter(fixed_text)\n",
    "        with open(os.path.join(out_path_lem, file), 'w', encoding='utf-8') as outfile:\n",
    "            outfile.write(filtered)\n",
    "    except Exception as e:\n",
    "        print(file, e)\n",
    "#összevonások kezelése és stemmelés, majd szövegek kimentése"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_folder = 'txts_stempos'\n",
    "in_path = os.path.join('Data', in_folder)\n",
    "in_files = [f for f in os.listdir(in_path) if os.path.isfile(os.path.join(in_path, f))]\n",
    "\n",
    "#összes fájl listába rakása\n",
    "txt_sigbig = []\n",
    "for filename in tqdm.tqdm(in_files):\n",
    "    try:\n",
    "        with open(os.path.join(in_path, filename), 'r', encoding='utf-8') as infile:\n",
    "            article = infile.read()\n",
    "        txt_sigbig.append(article)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "#szövegekből és szavakból álló listák készítése\n",
    "txts_sigbig = []\n",
    "for t in tqdm.tqdm(txt_sigbig):\n",
    "    txts_sigbig.append(t.strip().split())\n",
    "words_sigbig = list(itertools.chain(*txts_sigbig))\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "\n",
    "#függvények definiálása bigramok készítéséhez\n",
    "def bag_of_words(words):\n",
    "    return dict([(word, True) for word in words])\n",
    "\n",
    "\n",
    "#forrás: http://www.nltk.org/howto/collocations.html\n",
    "def bag_of_bigrams_words(words, score_fn=bigram_measures.pmi, n=500):\n",
    "    bigram_finder = BigramCollocationFinder.from_words(words)\n",
    "    bigram_finder.apply_freq_filter(min_freq=20)\n",
    "    bigrams = bigram_finder.nbest(score_fn, n)\n",
    "    return bag_of_words(bigrams)\n",
    "\n",
    "\n",
    "#bigramok kimentése fájlba\n",
    "bigrams_file = open(os.path.join('Data', 'significant_bigrams.tsv'), 'w', encoding='utf-8')\n",
    "\n",
    "#bigram detektálás\n",
    "detected_bigrams = bag_of_bigrams_words(words_sigbig)\n",
    "\n",
    "for bigram in tqdm.tqdm(detected_bigrams):\n",
    "    line = bigram[0] + ' ' + bigram[1] + \"\\t\" + bigram[0] + \"_\" + bigram[1] + '\\n'\n",
    "    bigrams_file.write(line)\n",
    "bigrams_file.close()\n",
    "\n",
    "#bigramok listájának betöltése\n",
    "bigram_list = open(os.path.join('Data', 'significant_bigrams_nr.tsv'), 'r',\n",
    "                   encoding='utf-8').readlines()\n",
    "\n",
    "\n",
    "#bigramok listába rakása\n",
    "bigrams = []\n",
    "for bigram in bigram_list:\n",
    "    w1, w2 = bigram.strip().split('\\t')\n",
    "    tup = (w1, w2)\n",
    "    bigrams.append(tup)\n",
    "\n",
    "\n",
    "#függvény bigramokok cseréjéhez\n",
    "def filter_bigrams(text, xgrams=bigrams):\n",
    "    text = \" \" + text + \" \"\n",
    "    for i in xgrams:\n",
    "        text = text.replace(\" \" + i[0] + \" \", \" \" + i[1] + \" \")\n",
    "    return text\n",
    "\n",
    "\n",
    "#fájlok törlése a mappában (ha vannak)\n",
    "out_path_sigbig = os.path.join('Data', 'txts_bigram')\n",
    "folder = [f for f in os.listdir(out_path_sigbig) if os.path.isfile(os.path.join(out_path_sigbig, f))]\n",
    "if len(folder) != 0:\n",
    "    for f in folder:\n",
    "        os.remove(os.path.join(out_path_sigbig, f))\n",
    "\n",
    "#bigramok cseréje\n",
    "file = 0\n",
    "for filename in tqdm.tqdm(in_files):\n",
    "    try:\n",
    "        article = open(os.path.join(in_path, filename), 'r', encoding='utf-8').read()\n",
    "        sigbigrams = filter_bigrams(article)\n",
    "        with open(os.path.join(out_path_sigbig, in_files[file]), 'w', encoding='utf-8') as out_file:\n",
    "            out_file.write(sigbigrams)\n",
    "    except Exception as e:\n",
    "        print(in_files[file], e)\n",
    "        continue\n",
    "    file += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_folder = 'txts_bigram'\n",
    "in_path = os.path.join('Data', in_folder)\n",
    "in_files = [f for f in os.listdir(in_path) if os.path.isfile(os.path.join(in_path, f))]\n",
    "\n",
    "\n",
    "#szövegek listázása a txts_bigram mappából\n",
    "txt = []\n",
    "for filename in tqdm.tqdm(in_files):\n",
    "    with open(os.path.join(in_path, filename), 'r', encoding='utf-8') as infile:\n",
    "        article = infile.read()\n",
    "    txt.append(article)\n",
    "\n",
    "\n",
    "out_path = os.path.join('Data', 'txts_final')\n",
    "\n",
    "#stopszavazás\n",
    "for filename in tqdm.tqdm(in_files):\n",
    "    try:\n",
    "        with open(os.path.join(in_path, filename), 'r', encoding='utf-8') as article:\n",
    "            text = article.read()\n",
    "        words = text.split()\n",
    "        article_clean = []\n",
    "        for word in words:\n",
    "            if nltk.pos_tag([word])[0][1] == 'PRP' or word not in stop_en:\n",
    "                article_clean.append(word)\n",
    "        with open(os.path.join(out_path, filename), 'w', encoding='utf-8') as outfile:\n",
    "            outfile.write(' '.join(article_clean))\n",
    "    except Exception as e:\n",
    "        print(filename, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"domain2\"].value_counts()\n",
    "#bejegyzések száma források szerint"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
